# LLM backend (vLLM or any OpenAI-compatible server)
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_API_KEY=not-required
LLM_MODEL=Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4

# Inference params
LLM_TEMPERATURE=0.1
LLM_MAX_RETRIES=2

# Path to prompts YAML (defaults to prompts.yaml next to main.py)
# PROMPTS_FILE=/path/to/custom-prompts.yaml
